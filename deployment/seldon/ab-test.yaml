# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è Seldon Core
apiVersion: machinelearning.seldon.io/v1
kind: SeldonDeployment
metadata:
  name: credit-scoring-ab-test
  namespace: credit-scoring-production
spec:
  name: credit-scoring-ab
  protocol: kfserving
  
  predictors:
    # –ú–æ–¥–µ–ª—å A (—Ç–µ–∫—É—â–∞—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω)
    - name: model-a
      graph:
        name: model-a
        type: MODEL
        implementation: TRITON_SERVER
        modelUri: "s3://models-registry-production/credit_scoring_quantized.onnx"
        parameters:
          - name: model_name
            type: STRING
            value: "credit_scoring_v1"
          - name: version
            type: STRING
            value: "v1.0.0"
      
      componentSpecs:
        - spec:
            containers:
              - name: model-a-container
                image: "nvcr.io/nvidia/tritonserver:22.12-py3"
                args:
                  - "tritonserver"
                  - "--model-repository=/mnt/models"
                  - "--http-port=9000"
                volumeMounts:
                  - name: models-volume-a
                    mountPath: /mnt/models
            initContainers:
              - name: model-a-initializer
                image: "seldonio/rclone-storage-initializer:1.14.0"
                args:
                  - "--action=download"
                  - "--src_path=s3://models-registry-production/credit_scoring_quantized.onnx"
                  - "--dest_path=/mnt/models"
                volumeMounts:
                  - name: models-volume-a
                    mountPath: /mnt/models
            volumes:
              - name: models-volume-a
                emptyDir: {}
      
      replicas: 2
      traffic: 50  # 50% —Ç—Ä–∞—Ñ–∏–∫–∞
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9000"
        prometheus.io/path: "/metrics"
        ab-test-version: "v1"
    
    # –ú–æ–¥–µ–ª—å B (–Ω–æ–≤–∞—è –≤–µ—Ä—Å–∏—è)
    - name: model-b
      graph:
        name: model-b
        type: MODEL
        implementation: TRITON_SERVER
        modelUri: "s3://models-registry-staging/credit_scoring_v2.onnx"
        parameters:
          - name: model_name
            type: STRING
            value: "credit_scoring_v2"
          - name: version
            type: STRING
            value: "v2.0.0-beta"
        children: []
      
      componentSpecs:
        - spec:
            containers:
              - name: model-b-container
                image: "nvcr.io/nvidia/tritonserver:22.12-py3"
                args:
                  - "tritonserver"
                  - "--model-repository=/mnt/models"
                  - "--http-port=9001"
                volumeMounts:
                  - name: models-volume-b
                    mountPath: /mnt/models
            initContainers:
              - name: model-b-initializer
                image: "seldonio/rclone-storage-initializer:1.14.0"
                args:
                  - "--action=download"
                  - "--src_path=s3://models-registry-staging/credit_scoring_v2.onnx"
                  - "--dest_path=/mnt/models"
                volumeMounts:
                  - name: models-volume-b
                    mountPath: /mnt/models
            volumes:
              - name: models-volume-b
                emptyDir: {}
      
      replicas: 2
      traffic: 50  # 50% —Ç—Ä–∞—Ñ–∏–∫–∞
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9001"
        prometheus.io/path: "/metrics"
        ab-test-version: "v2"

  # –°–µ—Ä–≤–∏—Å –¥–ª—è A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
  service:
    type: LoadBalancer
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "8000"

  # Traffic split configuration
  trafficSplit:
    - name: model-a
      weight: 50
      serviceName: credit-scoring-ab-test-model-a
    - name: model-b
      weight: 50
      serviceName: credit-scoring-ab-test-model-b

  # Metrics –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
  metrics:
    enabled: true
    endpoint: "http://prometheus.monitoring.svc.cluster.local:9090"
    interval: 30s
    metrics:
      - name: model_a_accuracy
        type: GAUGE
        help: "Accuracy of Model A"
        labels:
          model: "credit_scoring_v1"
      - name: model_b_accuracy
        type: GAUGE
        help: "Accuracy of Model B"
        labels:
          model: "credit_scoring_v2"
      - name: model_a_latency_p95
        type: GAUGE
        help: "P95 latency of Model A"
        labels:
          model: "credit_scoring_v1"
      - name: model_b_latency_p95
        type: GAUGE
        help: "P95 latency of Model B"
        labels:
          model: "credit_scoring_v2"

  # Analytics –¥–ª—è A/B —Ç–µ—Å—Ç–∞
  analytics:
    enabled: true
    interval: 60s
    metrics:
      - name: requests_total
        type: COUNTER
        endpoint: "/metrics"
      - name: success_rate
        type: GAUGE
        endpoint: "/metrics"
      - name: latency_ms
        type: GAUGE
        endpoint: "/metrics"
---
# ConfigMap –¥–ª—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ A/B —Ç–µ—Å—Ç–∞
apiVersion: v1
kind: ConfigMap
metadata:
  name: ab-test-config
  namespace: credit-scoring-production
data:
  ab-test-config.yaml: |
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    test_name: "credit_scoring_model_comparison"
    start_time: "2024-01-15T00:00:00Z"
    duration_days: 14
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    primary_metrics:
      - name: "accuracy"
        threshold: 0.85
        weight: 0.4
      - name: "precision"
        threshold: 0.8
        weight: 0.2
      - name: "recall"
        threshold: 0.75
        weight: 0.2
      - name: "latency_p95_ms"
        threshold: 100
        weight: 0.1
        lower_is_better: true
      - name: "throughput_rps"
        threshold: 100
        weight: 0.1
        lower_is_better: false
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    statistical_power: 0.8
    significance_level: 0.05
    minimum_sample_size: 10000
    
    # –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞
    winning_criteria:
      - metric: "composite_score"
        improvement: 0.05  # 5% —É–ª—É—á—à–µ–Ω–∏–µ
      - metric: "accuracy"
        improvement: 0.02  # 2% —É–ª—É—á—à–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏
      - no_regression: true  # –ù–∏–∫–∞–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –ø–æ –¥—Ä—É–≥–∏–º –º–µ—Ç—Ä–∏–∫–∞–º
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è
    auto_actions:
      - trigger: "model_b_wins"
        action: "promote_to_production"
        confidence: 0.95
      - trigger: "no_significant_difference"
        action: "extend_test"
        duration_days: 7
      - trigger: "model_b_loses"
        action: "rollback"
        notify: ["ml-team@company.com"]
    
    # –ù–æ—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
    notifications:
      - channel: "slack"
        webhook: "$SLACK_WEBHOOK_URL"
        events: ["test_started", "significant_result", "test_completed"]
      - channel: "email"
        recipients: ["data-science@company.com", "ml-ops@company.com"]
        events: ["test_completed", "action_required"]
---
# Service –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ A/B —Ç–µ—Å—Ç—É
apiVersion: v1
kind: Service
metadata:
  name: credit-scoring-ab-test-service
  namespace: credit-scoring-production
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8000"
spec:
  selector:
    app.kubernetes.io/name: credit-scoring-ab-test
  ports:
    - name: http
      port: 80
      targetPort: 8000
      protocol: TCP
    - name: metrics
      port: 9090
      targetPort: 9090
      protocol: TCP
  type: LoadBalancer
---
# –°–∫—Ä–∏–ø—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ A/B —Ç–µ—Å—Ç–∞
apiVersion: v1
kind: ConfigMap
metadata:
  name: ab-test-analysis-script
  namespace: credit-scoring-production
data:
  analyze_ab_test.py: |
    #!/usr/bin/env python3
    """
    –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∫—Ä–µ–¥–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∏–Ω–≥–∞
    """
    
    import pandas as pd
    import numpy as np
    import json
    from datetime import datetime, timedelta
    from scipy import stats
    import matplotlib.pyplot as plt
    from typing import Dict, List, Tuple
    import mlflow
    
    class ABTestAnalyzer:
        """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        
        def __init__(self, prometheus_url: str = "http://prometheus.monitoring.svc.cluster.local:9090"):
            self.prometheus_url = prometheus_url
            self.results = {}
            
        def collect_metrics(self, start_time: str, end_time: str) -> Dict:
            """–°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –∑–∞ –ø–µ—Ä–∏–æ–¥ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
            print(f"–°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ —Å {start_time} –ø–æ {end_time}")
            
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã –∑–∞–ø—Ä–æ—Å –∫ Prometheus API
            # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            
            np.random.seed(42)
            n_samples = 10000
            
            # –°–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è Model A
            model_a_metrics = {
                'accuracy': np.random.beta(85, 15, n_samples) / 100,  # ~85% —Ç–æ—á–Ω–æ—Å—Ç—å
                'precision': np.random.beta(80, 20, n_samples) / 100,
                'recall': np.random.beta(75, 25, n_samples) / 100,
                'latency_ms': np.random.exponential(50, n_samples),
                'throughput_rps': np.random.normal(120, 20, n_samples)
            }
            
            # –°–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è Model B (–Ω–µ–º–Ω–æ–≥–æ –ª—É—á—à–µ)
            model_b_metrics = {
                'accuracy': np.random.beta(87, 13, n_samples) / 100,  # ~87% —Ç–æ—á–Ω–æ—Å—Ç—å
                'precision': np.random.beta(82, 18, n_samples) / 100,
                'recall': np.random.beta(78, 22, n_samples) / 100,
                'latency_ms': np.random.exponential(45, n_samples),  # –ë—ã—Å—Ç—Ä–µ–µ
                'throughput_rps': np.random.normal(130, 15, n_samples)  # –í—ã—à–µ throughput
            }
            
            return {
                'model_a': model_a_metrics,
                'model_b': model_b_metrics,
                'sample_size': n_samples,
                'period': f"{start_time} to {end_time}"
            }
        
        def calculate_statistical_significance(self, metrics_a: Dict, metrics_b: Dict) -> Dict:
            """–†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ —Ä–∞–∑–ª–∏—á–∏–π"""
            print("–†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏...")
            
            significance_results = {}
            
            for metric_name in metrics_a.keys():
                a_data = metrics_a[metric_name]
                b_data = metrics_b[metric_name]
                
                # T-test –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–∏—Ö
                t_stat, p_value = stats.ttest_ind(a_data, b_data, equal_var=False)
                
                # –†–∞–∑–Ω–∏—Ü–∞ —Å—Ä–µ–¥–Ω–∏—Ö
                mean_a = np.mean(a_data)
                mean_b = np.mean(b_data)
                mean_diff = mean_b - mean_a
                percent_diff = (mean_diff / mean_a) * 100 if mean_a != 0 else 0
                
                # –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
                ci_low, ci_high = stats.t.interval(
                    0.95, 
                    len(a_data) + len(b_data) - 2,
                    loc=mean_diff,
                    scale=stats.sem(np.concatenate([a_data - mean_a, b_data - mean_b]))
                )
                
                significance_results[metric_name] = {
                    'mean_a': float(mean_a),
                    'mean_b': float(mean_b),
                    'mean_difference': float(mean_diff),
                    'percent_difference': float(percent_diff),
                    'p_value': float(p_value),
                    'significant': p_value < 0.05,
                    'confidence_interval': [float(ci_low), float(ci_high)],
                    'effect_size': float(mean_diff / np.std(np.concatenate([a_data, b_data])))
                }
            
            return significance_results
        
        def calculate_composite_score(self, metrics: Dict, weights: Dict) -> float:
            """–†–∞—Å—á–µ—Ç –∫–æ–º–ø–æ–∑–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∞"""
            composite_score = 0
            total_weight = 0
            
            for metric_name, weight in weights.items():
                if metric_name in metrics:
                    metric_value = metrics[metric_name]['mean_b']
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
                    if metric_name == 'latency_ms':
                        # –î–ª—è latency –º–µ–Ω—å—à–µ = –ª—É—á—à–µ
                        normalized = 1 / (1 + metric_value / 100)
                    elif metric_name == 'throughput_rps':
                        # –î–ª—è throughput –±–æ–ª—å—à–µ = –ª—É—á—à–µ
                        normalized = metric_value / 200
                    else:
                        # –î–ª—è accuracy, precision, recall —É–∂–µ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 0-1
                        normalized = metric_value
                    
                    composite_score += normalized * weight
                    total_weight += weight
            
            return composite_score / total_weight if total_weight > 0 else 0
        
        def run_analysis(self, config_path: str = "/etc/ab-test/config.yaml") -> Dict:
            """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ A/B —Ç–µ—Å—Ç–∞"""
            print("–ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è...")
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            import yaml
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            
            # –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫
            end_time = datetime.now()
            start_time = end_time - timedelta(days=config.get('duration_days', 14))
            
            metrics = self.collect_metrics(
                start_time.isoformat(),
                end_time.isoformat()
            )
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
            significance = self.calculate_statistical_significance(
                metrics['model_a'],
                metrics['model_b']
            )
            
            # –†–∞—Å—á–µ—Ç –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã—Ö —Å–∫–æ—Ä–æ–≤
            weights = {m['name']: m['weight'] for m in config['primary_metrics']}
            composite_score_a = self.calculate_composite_score(
                {k: {'mean_b': v['mean_a']} for k, v in significance.items()},
                weights
            )
            composite_score_b = self.calculate_composite_score(significance, weights)
            
            # –ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è
            decision = self.make_decision(significance, composite_score_a, composite_score_b, config)
            
            # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self.visualize_results(metrics, significance, config)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            results = {
                'analysis_timestamp': datetime.now().isoformat(),
                'test_config': config,
                'metrics_collected': metrics,
                'statistical_analysis': significance,
                'composite_scores': {
                    'model_a': composite_score_a,
                    'model_b': composite_score_b,
                    'improvement': composite_score_b - composite_score_a
                },
                'decision': decision,
                'recommendations': self.generate_recommendations(decision, significance)
            }
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ MLflow
            self.log_to_mlflow(results)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
            self.save_report(results)
            
            print(f"–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω. –†–µ—à–µ–Ω–∏–µ: {decision['verdict']}")
            return results
        
        def make_decision(self, significance: Dict, score_a: float, score_b: float, config: Dict) -> Dict:
            """–ü—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏—è –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º —Ç–µ—Å—Ç–∞"""
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞
            winning_criteria = config.get('winning_criteria', [])
            criteria_met = []
            
            for criterion in winning_criteria:
                if criterion.get('metric') == 'composite_score':
                    improvement_needed = criterion.get('improvement', 0)
                    actual_improvement = score_b - score_a
                    criteria_met.append({
                        'criterion': 'composite_score',
                        'required': improvement_needed,
                        'actual': actual_improvement,
                        'met': actual_improvement >= improvement_needed
                    })
                
                elif criterion.get('metric') == 'accuracy':
                    improvement_needed = criterion.get('improvement', 0)
                    accuracy_improvement = significance['accuracy']['percent_difference'] / 100
                    criteria_met.append({
                        'criterion': 'accuracy',
                        'required': improvement_needed,
                        'actual': accuracy_improvement,
                        'met': accuracy_improvement >= improvement_needed
                    })
                
                elif criterion.get('no_regression', False):
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º –º–µ—Ç—Ä–∏–∫–∞–º
                    regressions = []
                    for metric_name, result in significance.items():
                        if metric_name in ['accuracy', 'precision', 'recall']:
                            if result['mean_b'] < result['mean_a'] and result['significant']:
                                regressions.append(metric_name)
                    
                    criteria_met.append({
                        'criterion': 'no_regression',
                        'regressions_found': regressions,
                        'met': len(regressions) == 0
                    })
            
            # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ
            all_criteria_met = all([c['met'] for c in criteria_met])
            significant_improvement = (score_b - score_a) > 0.05  # 5% —É–ª—É—á—à–µ–Ω–∏–µ
            
            if all_criteria_met and significant_improvement:
                verdict = "MODEL_B_WINS"
                action = "promote_to_production"
                confidence = "high"
            elif not any([c['met'] for c in criteria_met]):
                verdict = "MODEL_A_WINS"
                action = "keep_current"
                confidence = "high"
            else:
                verdict = "INCONCLUSIVE"
                action = "extend_test"
                confidence = "medium"
            
            return {
                'verdict': verdict,
                'action': action,
                'confidence': confidence,
                'criteria_evaluation': criteria_met,
                'composite_score_improvement': score_b - score_a
            }
        
        def generate_recommendations(self, decision: Dict, significance: Dict) -> List[str]:
            """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
            recommendations = []
            
            if decision['verdict'] == "MODEL_B_WINS":
                recommendations.append("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—å Model B –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω")
                recommendations.append("–û–±–Ω–æ–≤–∏—Ç—å –∫–æ–Ω–≤–µ–π–µ—Ä CI/CD –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏")
                recommendations.append("–£–≤–µ–¥–æ–º–∏—Ç—å –∫–æ–º–∞–Ω–¥—É data science –æ —É—Å–ø–µ—à–Ω–æ–º —Ç–µ—Å—Ç–µ")
            
            elif decision['verdict'] == "MODEL_A_WINS":
                recommendations.append("‚úÖ –û—Å—Ç–∞–≤–∏—Ç—å —Ç–µ–∫—É—â—É—é Model A –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ")
                recommendations.append("–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏—á–∏–Ω—ã –Ω–µ—É–¥–∞—á–∏ Model B")
                recommendations.append("–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è")
            
            else:  # INCONCLUSIVE
                recommendations.append("‚è≥ –ü—Ä–æ–¥–ª–∏—Ç—å A/B —Ç–µ—Å—Ç –Ω–∞ 7 –¥–Ω–µ–π")
                recommendations.append("–£–≤–µ–ª–∏—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –±–æ–ª—å—à–µ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –º–æ—â–Ω–æ—Å—Ç–∏")
                recommendations.append("–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å—é")
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º
            for metric_name, result in significance.items():
                if result['significant'] and abs(result['percent_difference']) > 10:
                    direction = "—É–ª—É—á—à–∏–ª–∞—Å—å" if result['percent_difference'] > 0 else "—É—Ö—É–¥—à–∏–ª–∞—Å—å"
                    recommendations.append(
                        f"üìä –ú–µ—Ç—Ä–∏–∫–∞ '{metric_name}' {direction} –Ω–∞ {abs(result['percent_difference']):.1f}%"
                    )
            
            return recommendations
        
        def visualize_results(self, metrics: Dict, significance: Dict, config: Dict):
            """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ A/B —Ç–µ—Å—Ç–∞"""
            import matplotlib.pyplot as plt
            
            fig, axes = plt.subplots(2, 3, figsize=(15, 10))
            fig.suptitle('A/B Test Results: Credit Scoring Models', fontsize=16)
            
            metrics_to_plot = list(metrics['model_a'].keys())[:6]
            
            for idx, metric_name in enumerate(metrics_to_plot):
                ax = axes[idx // 3, idx % 3]
                
                # –î–∞–Ω–Ω—ã–µ –¥–ª—è box plot
                data_a = metrics['model_a'][metric_name]
                data_b = metrics['model_b'][metric_name]
                
                bp = ax.boxplot([data_a, data_b], labels=['Model A', 'Model B'], patch_artist=True)
                
                # –†–∞—Å–∫—Ä–∞—à–∏–≤–∞–µ–º box plot
                colors = ['lightblue', 'lightgreen']
                for patch, color in zip(bp['boxes'], colors):
                    patch.set_facecolor(color)
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å
                p_value = significance[metric_name]['p_value']
                if p_value < 0.05:
                    # –î–æ–±–∞–≤–ª—è–µ–º –∑–≤–µ–∑–¥–æ—á–∫—É –¥–ª—è –∑–Ω–∞—á–∏–º—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π
                    y_max = max(np.max(data_a), np.max(data_b))
                    ax.text(1.5, y_max * 1.05, f'p = {p_value:.3f}*', 
                           ha='center', fontweight='bold', color='red')
                
                ax.set_title(f'{metric_name.capitalize()}')
                ax.set_ylabel(metric_name)
                ax.grid(True, alpha=0.3)
            
            # –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π —Å–∫–æ—Ä
            ax = axes[1, 2]
            composite_scores = [0.75, 0.82]  # –ü—Ä–∏–º–µ—Ä–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            bars = ax.bar(['Model A', 'Model B'], composite_scores, color=['lightblue', 'lightgreen'])
            ax.set_title('Composite Score')
            ax.set_ylabel('Score')
            ax.set_ylim([0, 1])
            
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –±–∞—Ä—ã
            for bar, score in zip(bars, composite_scores):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{score:.3f}', ha='center', va='bottom')
            
            plt.tight_layout()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫
            plt.savefig('/tmp/ab_test_results.png', dpi=150, bbox_inches='tight')
            plt.close()
            
            print("–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: /tmp/ab_test_results.png")
        
        def log_to_mlflow(self, results: Dict):
            """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ MLflow"""
            try:
                with mlflow.start_run(run_name="ab_test_analysis"):
                    # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏
                    mlflow.log_metric("composite_score_a", results['composite_scores']['model_a'])
                    mlflow.log_metric("composite_score_b", results['composite_scores']['model_b'])
                    mlflow.log_metric("improvement", results['composite_scores']['improvement'])
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –∑–Ω–∞—á–∏–º–æ—Å—Ç—å
                    for metric_name, result in results['statistical_analysis'].items():
                        mlflow.log_metric(f"{metric_name}_p_value", result['p_value'])
                        mlflow.log_metric(f"{metric_name}_improvement", result['percent_difference'])
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ—à–µ–Ω–∏–µ
                    mlflow.log_param("ab_test_verdict", results['decision']['verdict'])
                    mlflow.log_param("recommended_action", results['decision']['action'])
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
                    mlflow.log_dict(results, "ab_test_report.json")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –≥—Ä–∞—Ñ–∏–∫
                    mlflow.log_artifact("/tmp/ab_test_results.png")
                    
                print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞–Ω—ã –≤ MLflow")
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤ MLflow: {e}")
        
        def save_report(self, results: Dict):
            """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞"""
            report_path = "/tmp/ab_test_report.json"
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            print(f"–û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
            
            # –¢–∞–∫–∂–µ —Å–æ–∑–¥–∞–µ–º —á–∏—Ç–∞–±–µ–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é
            self.create_human_readable_report(results)
        
        def create_human_readable_report(self, results: Dict):
            """–°–æ–∑–¥–∞–Ω–∏–µ —á–∏—Ç–∞–±–µ–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
            report = f"""
            ========================================
            A/B TEST REPORT: CREDIT SCORING MODELS
            ========================================
            
            –î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {results['analysis_timestamp']}
            
            –†–ï–ó–Æ–ú–ï:
            ---------
            –í–µ—Ä–¥–∏–∫—Ç: {results['decision']['verdict']}
            –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ: {results['decision']['action']}
            –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {results['decision']['confidence']}
            
            –ö–û–ú–ü–û–ó–ò–¢–ù–´–ï –°–ö–û–†–´:
            ------------------
            Model A: {results['composite_scores']['model_a']:.3f}
            Model B: {results['composite_scores']['model_b']:.3f}
            –£–ª—É—á—à–µ–Ω–∏–µ: {results['composite_scores']['improvement']:.3f} ({results['composite_scores']['improvement']*100:.1f}%)
            
            –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó:
            ----------------------
            """
            
            for metric_name, result in results['statistical_analysis'].items():
                significance = "‚úÖ –ó–ù–ê–ß–ò–ú–û" if result['significant'] else "‚ùå –ù–ï–ó–ù–ê–ß–ò–ú–û"
                direction = "‚Üë —É–ª—É—á—à–µ–Ω–∏–µ" if result['percent_difference'] > 0 else "‚Üì —É—Ö—É–¥—à–µ–Ω–∏–µ"
                
                report += f"""
            {metric_name.upper()}:
              ‚Ä¢ Model A: {result['mean_a']:.3f}
              ‚Ä¢ Model B: {result['mean_b']:.3f}
              ‚Ä¢ –†–∞–∑–Ω–∏—Ü–∞: {result['mean_difference']:.3f} ({result['percent_difference']:.1f}%) {direction}
              ‚Ä¢ p-value: {result['p_value']:.4f} {significance}
              ‚Ä¢ –î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª: [{result['confidence_interval'][0]:.3f}, {result['confidence_interval'][1]:.3f}]
                """
            
            report += f"""
            
            –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:
            -------------
            """
            
            for i, rec in enumerate(results['recommendations'], 1):
                report += f"{i}. {rec}\n"
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
            with open("/tmp/ab_test_human_report.txt", 'w', encoding='utf-8') as f:
                f.write(report)
            
            print(f"–ß–∏—Ç–∞–±–µ–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: /tmp/ab_test_human_report.txt")
    
    # –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è
    if __name__ == "__main__":
        analyzer = ABTestAnalyzer()
        results = analyzer.run_analysis()
        
        print("\n" + "="*60)
        print("–ê–Ω–∞–ª–∏–∑ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω")
        print("="*60)
        print(f"–í–µ—Ä–¥–∏–∫—Ç: {results['decision']['verdict']}")
        print(f"–î–µ–π—Å—Ç–≤–∏–µ: {results['decision']['action']}")
        print(f"–£–ª—É—á—à–µ–Ω–∏–µ –∫–æ–º–ø–æ–∑–∏—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∞: {results['composite_scores']['improvement']*100:.1f}%")
        print("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        for rec in results['recommendations']:
            print(f"  ‚Ä¢ {rec}")